Fitts's law is a model of speed-accuracy tradeoffs used in humanâ€“computer interaction and
ergonomics. It predicts time required to acquire a target on screen as a function of the distance
to the target and the size of the target. Fitts's law is used to model the act of pointing, either by
physically touching an object with a hand,finger or virtually or by pointing to an object on a
computer monitor using a pointing device. It was proposed by Paul Fitts in 1954.
Mathematically it can be written as

MT = a + b log 2 ( 2A / W )

MT : Movement time (average) taken to complete the movement or point the target.
a : Start / Stop time of the device (y intercept)
b : Inherent speed of the device (slope of line)
W : Width of the target measured along the axis of motion, which corresponds to accuracy
A : Distance from the starting point to the center of the target